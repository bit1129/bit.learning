从Spark官网上下载的Spark安装包已经内置了Hive，
为了能够在Spark中访问Hive的元数据以及对Hive的表进行查询，需要做如下配置
1. 将hive的hive-site.xml复制到spark的conf目录下
更正规的做法是建立软链，

ln -s $HIVE_HOME/conf/hive-site.xml $SPARK_HOME/conf/hive-site.xml

2. 将mysql的驱动添加到spark应用的classpath上（具体的是指Driver和Executor）

方法有两种：
a. 在spark-env.sh中添加如下配置：
SPARK_CLASSPATH=/home/yuzt/software/bigdata/spark-1.6.1-bin-hadoop2.6/lib/mysql-connector-java-5.1.38.jar:$SPARK_CLASSPATH

因为SPARK_CLASSPATH已经不推荐使用，所以建议使用第二种方法
b. 在spark-defaults.conf添加如下配置
spark.driver.extraClassPath=/home/yuzt/software/bigdata/spark-1.6.1-bin-hadoop2.6/lib/mysql-connector-java-5.1.38.jar
spark.executor.extraClassPath=/home/yuzt/software/bigdata/spark-1.6.1-bin-hadoop2.6/lib/mysql-connector-java-5.1.38.jar