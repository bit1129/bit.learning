org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.12.167.42:50882 --executor-id 0 --hostname 10.12.167.42 --cores 4 --app-id app-20160202111157-0002 --worker-url spark://Worker@10.12.167.42:57390



启动Spark Standalone mode，

在Application运行过程中，杀死CoarseGrainedExecutorBackend，此时，该进程会很快恢复,此时Driver控制台抛出如下日志



16/02/02 11:12:40 ERROR scheduler.TaskSchedulerImpl: Lost executor 0 on 10.12.167.42: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
16/02/02 11:12:40 INFO scheduler.DAGScheduler: Executor lost: 0 (epoch 0)
16/02/02 11:12:40 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
16/02/02 11:12:40 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.12.167.42, 32912)
16/02/02 11:12:40 INFO storage.BlockManagerMaster: Removed 0 successfully in removeExecutor
16/02/02 11:12:40 INFO client.AppClient$ClientEndpoint: Executor updated: app-20160202111157-0002/0 is now EXITED (Command exited with code 137)
16/02/02 11:12:40 INFO cluster.SparkDeploySchedulerBackend: Executor app-20160202111157-0002/0 removed: Command exited with code 137
16/02/02 11:12:40 INFO cluster.SparkDeploySchedulerBackend: Asked to remove non-existent executor 0
16/02/02 11:12:40 INFO client.AppClient$ClientEndpoint: Executor added: app-20160202111157-0002/1 on worker-20160202110416-10.12.167.42-57390 (10.12.167.42:57390) with 4 cores
16/02/02 11:12:40 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20160202111157-0002/1 on hostPort 10.12.167.42:57390 with 4 cores, 1024.0 MB RAM
16/02/02 11:12:40 INFO client.AppClient$ClientEndpoint: Executor updated: app-20160202111157-0002/1 is now RUNNING
16/02/02 11:12:43 INFO cluster.SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (10.12.167.42:56828) with ID 1
16/02/02 11:12:43 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.12.167.42:33794 with 511.5 MB RAM, BlockManagerId(1, 10.12.167.42, 33794














package com.bit

import org.apache.spark.{SparkConf, SparkContext}

object WordcountTest2 {
  def main(args: Array[String]): Unit = {

    val sleepSeconds = if (args == null || args.length <= 0) 1 else args(0).toInt;

    println("The WordcountTest started to run")
    val conf = new SparkConf().setAppName("WordcountTest")
    val master = conf.getOption("spark.master") match {
      case Some(master) => master
      case _ => "local"
    }
    conf.setMaster(master)
    val sc = new SparkContext(conf)
    val rdd = sc.parallelize(List(1, 2, 3, 4, 5, 6))
    println(s"Waiting ${1000 * sleepSeconds} milliseconds to continue")
    Thread.sleep(1000 * sleepSeconds)
    val sum = rdd.reduce(_ + _)
    println(s"The sum is : $sum")
    sc.stop()

  }
}