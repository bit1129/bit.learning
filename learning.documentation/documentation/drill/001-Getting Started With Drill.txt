0. 参考资料：
http://drill.apache.org/docs/architecture-introduction/
https://www.mapr.com/blog/apache-drill-architecture-ultimate-guide
http://www.slideshare.net/Hadoop_Summit/understanding-the-value-and-architecture-of-apache-drill
源码剖析
https://segmentfault.com/t/apache-drill/blogs
https://segmentfault.com/t/drill/blogs
https://segmentfault.com/a/1190000002912140
http://yangyoupeng-cn-fujitsu-com.iteye.com/blog/1974556
https://segmentfault.com/blog/drilldexuexi
http://www.confusedcoders.com/category/bigdata/apache-drill
http://www.confusedcoders.com/bigdata/apache-drill/understanding-apache-drill-logical-plan
https://segmentfault.com/a/1190000004325125
https://segmentfault.com/a/1190000004313643

https://www.mapr.com/blog/comparing-sql-functions-and-performance-apache-spark-and-apache-drill
https://www.mapr.com/blog/apache-spark-vs-apache-drill
https://www.dezyre.com/article/spark-sql-vs-apache-drill-war-of-the-sql-on-hadoop-tools/234


1. Apache Calcite 介绍
http://blog.csdn.net/yunlong34574/article/details/46375733
内容：
a. Apache Calcite介绍
b. 怎么基于Apache Calcite编写SQL on JavaBeans
c. Drill的逻辑结构图

总结：有了Calcite就有了前端的SQL接口了，然后增加上ZooKeeper，分布式的执行引擎和对应的Adapter，以及存储层的接口，就完成了一个分布式SQL执行引擎。其实这就是Apache Drill做的事情。Calcite中就有Spark与Calcite对接的部分代码，如果底层使用Spark那么，给Spark带来的就是ANSI 2003 SQL标准的接口了。对现在的状况会有很大的改善。

2. Drill编译器结构：
http://www.liuhaihua.cn/archives/2319.html

3. Drill源码剖析
http://www.liuhaihua.cn/archives/102356.html

4.启动Drill Embedded模式：
http://drill.apache.org/docs/starting-drill-on-linux-and-mac-os-x/

5.启动Drill Web Console:
http://drill.apache.org/docs/starting-the-web-console/
默认端口号是8047（访问地址：http://localhost:8047/）


6.Drill有哪些Storage Plugin：
Drill内置了如下Storage Plugin
cp
dfg
hbase
hive
kudu
mongo
s3
Drill支持Storage Plugin扩展

更新Storage Plugin的方法
http://drill.apache.org/docs/plugin-configuration-basics/#using-the-drill-web-console



7. Drill Embedded查询Hive表的步骤
http://drill.apache.org/docs/hive-storage-plugin/
http://drill.apache.org/docs/querying-hive/
http://drill.apache.org/docs/use/
a.启用Hive Storage Plugin
在http://localhost:8047/上编辑hive storage plugin

{
  "type": "hive",
  "enabled": true,
  "configProps": {
    "hive.metastore.uris": "thrift://localhost:9083",
    "javax.jdo.option.ConnectionURL": "jdbc:mysql://127.0.0.1:3306/hive_metastore?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useUnicode=true",
    "hive.metastore.warehouse.dir": "/user/hive/warehouse",
    "fs.default.name": "hdfs://hadoop.bit.com:9000",
    "hive.metastore.sasl.enabled": "false"
  }
}

b.执行bin/drill-embeded命令进行0: jdbc:drill:zk=local> 命令行模式
c.输入show schemas命令，可以看到列出的启动的storage plugin的schema信息
d.输入命令use hive就可以执行hive库的查询了

说明：这里的hive指的是数据源的名称，比如








8.什么是Drillbit？


9. Drill的系统架构图是怎么样的？伪分布式和真正分布式


10.什么是Drill的Plugin？





12.省略schema
0: jdbc:drill:zk=local> select * from dfs.`/home/yuzt/software/devsoftware/apache-drill-1.6.0/sample-data/region.parquet`;
可以改写为：
0: jdbc:drill:zk=local>use dfs;
0: jdbc:drill:zk=local> select * from `/home/yuzt/software/devsoftware/apache-drill-1.6.0/sample-data/region.parquet`;

注意：``不能省略

13. 问题：
在WEB UI上可以看到，dfs storage plugin默认使用的本地文件路径file:///
如何查询HDFS上的数据？

创建基于HDFS的Storage Plugin，参考：http://drill.apache.org/docs/file-system-storage-plugin/

{
  "type": "file",
  "enabled": true,
  "connection": "hdfs://hadoop.bit.com:9000/",
  "config": null,
  "workspaces": {
    "root": {
      "location": "/",
      "writable": true,
      "defaultInputFormat": null
    },
    "user": {
      "location": "/user",
      "writable": true,
      "defaultInputFormat": null
    }
  },
  "formats": {
    "json": {
      "type": "json",
      "extensions": [
        "json"
      ]
    }
  }
}

在命令行终端执行：show schemas可以看到hdfs这个schema

上传文件到HDFS，
hdfs dfs -put /home/yuzt/development/openprojects/spark-2.0.0-snapshot/examples/src/main/resources/people.json /
hdfs dfs -put /home/yuzt/development/openprojects/spark-2.0.0-snapshot/examples/src/main/resources/people.json /user

如下语句正确执行，如下语句应该等价于：select * from hdfs.`root`.`/people.json`
select * from hdfs.`/people.json`

如下语句失败：
select * from hdfs.`/people.json`
正确的写法是,如下操作是从/user/people.json读取数据
select * from hdfs.`user`.`/people.json`


14. Drill查询HBase数据库


15. order by group by语句

select name, avg(age) from hdfs.`/people.json` group by name order by name limit 2

查询的物理计划：
00-00    Screen : rowType = RecordType(ANY name, ANY EXPR$1): rowcount = 2.0, cumulative cost = {8.2 rows, 31.2 cpu, 0.0 io, 0.0 network, 16.0 memory}, id = 2136
00-01      Project(name=[$0], EXPR$1=[$1]) : rowType = RecordType(ANY name, ANY EXPR$1): rowcount = 2.0, cumulative cost = {8.0 rows, 31.0 cpu, 0.0 io, 0.0 network, 16.0 memory}, id = 2135
00-02        SelectionVectorRemover : rowType = RecordType(ANY name, ANY EXPR$1): rowcount = 2.0, cumulative cost = {8.0 rows, 31.0 cpu, 0.0 io, 0.0 network, 16.0 memory}, id = 2134
00-03          Limit(fetch=[2]) : rowType = RecordType(ANY name, ANY EXPR$1): rowcount = 2.0, cumulative cost = {6.0 rows, 29.0 cpu, 0.0 io, 0.0 network, 16.0 memory}, id = 2133
00-04            SelectionVectorRemover : rowType = RecordType(ANY name, ANY EXPR$1): rowcount = 1.0, cumulative cost = {4.0 rows, 21.0 cpu, 0.0 io, 0.0 network, 16.0 memory}, id = 2132
00-05              TopN(limit=[2]) : rowType = RecordType(ANY name, ANY EXPR$1): rowcount = 1.0, cumulative cost = {3.0 rows, 20.0 cpu, 0.0 io, 0.0 network, 16.0 memory}, id = 2131
00-06                StreamAgg(group=[{0}], EXPR$1=[MAX($1)]) : rowType = RecordType(ANY name, ANY EXPR$1): rowcount = 1.0, cumulative cost = {2.0 rows, 16.0 cpu, 0.0 io, 0.0 network, 16.0 memory}, id = 2130
00-07                  Sort(sort0=[$0], dir0=[ASC]) : rowType = RecordType(ANY name, ANY age): rowcount = 1.0, cumulative cost = {1.0 rows, 0.0 cpu, 0.0 io, 0.0 network, 16.0 memory}, id = 2129
00-08                    Scan(groupscan=[EasyGroupScan [selectionRoot=hdfs://hadoop.bit.com:9000/people.json, numFiles=1, columns=[`name`, `age`], files=[hdfs://hadoop.bit.com:9000/people.json]]]) : rowType = RecordType(ANY name, ANY age): rowcount = 1.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 2128


15. Drill怎么查询HBase？
a. 启动HBase，Zookeeper使用内置的, HBase的WEB UI地址是多少？http://localhost:16010
b. 在Drill的web ui上更新Hbase Storage plugin的配置
{
  "type": "hbase",
  "config": {
    "hbase.zookeeper.quorum": "localhost",
    "hbase.zookeeper.property.clientPort": "2181"
  },
  "size.calculator.enabled": false,
  "enabled": true
}

c. 假如HBase中有表t1,那么可以做如下查询
select * from hbase.`t1`


16. 一句话介绍Drill和Spark
Apache Drill – an open source, low latency SQL query engine for Hadoop and NoSQL.
Apache Spark – a general-purpose engine for large-scale data processing.

17. Drill Join
0: jdbc:drill:zk=local> select a.full_name from cp.`employee.json` as a join cp.`employee.json` as b on a.employee_id = b.employee_id limit 3 ;

上面是两个表进行self join

18. Performance between Drill and Spark
https://www.mapr.com/blog/comparing-sql-functions-and-performance-apache-spark-and-apache-drill
https://github.com/m-kiuchi/MovieLensSQL

19. 使用sqlline启动Drill
./sqlline -u jdbc:drill:zk=local

-u表示jdbc的url，对于Drill而言，它的jdbc串是jdbc:drill.zk

























